seed: 2222
clean: False
debug: False
train: True
result: evaluate  # evaluate, instance_numbers, sagittalcenters
load_directory: #/home/phalendj/code/rsna2024/outputs/2024-08-20/09-45-43  # For reloads
mode: valid

directories:
  relative_directory: /data/phalendj/kaggle/rsna2024
  image_directory: train_images


dataset:
  name: FullLevelDataset
  image_size: [512, 512]
  conditions: [Spinal Canal Stenosis, Left Neural Foraminal Narrowing, Right Neural Foraminal Narrowing, Left Subarticular Stenosis, Right Subarticular Stenosis]
  # conditions: [Spinal Canal Stenosis]
  sagittal_channels: 11
  sagittal_subsize: 64
  axial_channels: 7
  axial_subsize: 80
  aug_size: 0.1
  # center_file: /home/phalendj/code/rsna2024/outputs/2024-08-16/15-40-19/all_predicted_center_coordinates.csv
  # center_file: /data/phalendj/kaggle/rsna2024/train_labels_sagittal_t1.csv
  center_file: /data/phalendj/kaggle/rsna2024/train_label_coordinates.csv
  augmentations:
    prob: 0.75
    normalize: True  # Helpful with generalization
    hflip: False   # Very hurtful usually - cannot segment properly and everything comes in standard locations
    contrast: True  # Not much change
    blur: True   # This is a major adjustment that helps with training.
    distort: True  # This is a major adjustment that helps with training.  This one does the most to keep training and val in line, however it may prevent training from getting too good 
    rotate: True  # This is a major adjustment that helps with training.
    crop:   # 0.9 Does not do much - Maybe crop more? 0.7 hurts
      use: False
      size: 512
    channel_shuffle: False  # Removes all the information for TDCNN
    sharpen: False
  
  

loss:
  name: WeightedCrossEntropy
  # name: InstanceCrossEntropy
  # name: SevereLoss
  # name: HeatmapLoss
  # patch_size: 512
  
model:
  name: FullLevelTDCNN
  model_name: densenet121
  nclasses: 15
  num_layers: 4

optimization:
  optimizer: AdamW
  scheduler: CosineWithWarmup
  learning_rate: 3.0e-5
  weight_decay: 1.e-2
  n_cycles: 0.475

training:
  use_folds: [0, 1, 2, 3, 4]
  # use_folds: [2]
  use_amp: True
  epochs: 25
  batch_size: 24
  grad_acc: 1
  early_stopping: 5
  workers: 12

