seed: 2222
clean: 6
debug: False
train: True
result: evaluate  # evaluate, instance_numbers, sagittalcenters
load_directory: # /home/phalendj/code/rsna2024/outputs/2024-08-29/14-29-59  # For reloads
mode: valid

directories:
  relative_directory: /data/phalendj/kaggle/rsna2024
  image_directory: train_images


dataset:
  name: LevelCubeLeftRightAreaDataset
  image_size: [512, 512]
  
  # conditions: [Spinal Canal Stenosis, Left Neural Foraminal Narrowing, Right Neural Foraminal Narrowing, Left Subarticular Stenosis, Right Subarticular Stenosis]
  series_description: Axial T2
  conditions: [Left Subarticular Stenosis, Right Subarticular Stenosis]  # Need clean level 6
  channels: 7
  subsize: 224
  d_side: 40
  aug_size: 0.1
  center_file: /data/phalendj/kaggle/rsna2024/train_label_coordinates.csv
  augmentations:
    prob: 0.75
    normalize: True  # Helpful with generalization
    hflip: False   # Very hurtful usually - cannot segment properly and everything comes in standard locations
    contrast: True  # Not much change
    blur: True   # This is a major adjustment that helps with training.
    blur_limit: 9
    noise: True
    downscale: True
    distort: True  # This is a major adjustment that helps with training.  This one does the most to keep training and val in line, however it may prevent training from getting too good, especially for axial
    rotate: True  # This is a major adjustment that helps with training.
    crop:   # 0.9 Does not do much - Maybe crop more? 0.7 hurts
      use: False
      size: 512
    channel_shuffle: False  # Removes all the information for TDCNN
    sharpen: False
  
  

loss:
  name: WeightedCrossEntropy
  # name: InstanceCrossEntropy
  # name: SevereLoss
  # name: HeatmapLoss
  # patch_size: 512
  
model:
  name: tdcnnlevelside2
  model_name: densenet121
  # model_name: tf_efficientnetv2_b2.in1k
  nclasses: 3  # 6 with L/R, 3 for spinal
  num_layers: 4
  subsize: 64
  dropout: 0.1
  # preload: # /home/phalendj/code/rsna2024/outputs/2024-08-26/12-08-32/

optimization:
  optimizer: AdamW
  learning_rate: 1.0e-4
  weight_decay: 1.e-2

  # scheduler: CosineWithWarmup
  # n_cycles: 0.475

  scheduler: MultiStepLR
  milestones: [5, 10]
  gamma: 0.1

training:
  use_folds: [0, 1, 2, 3, 4]
  # use_folds: [2]
  use_amp: True
  epochs: 15
  batch_size: 6
  grad_acc: 1
  early_stopping: 5
  workers: 12

